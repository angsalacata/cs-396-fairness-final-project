# -*- coding: utf-8 -*-
"""RF_396.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FCbkehwrMSuBP-M0b_LP_dJ-eAOp-ZNe
"""

pip install ucimlrepo

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

from ucimlrepo import fetch_ucirepo

# fetch dataset
diabetes_130_us_hospitals_for_years_1999_2008 = fetch_ucirepo(id=296)

# data (as pandas dataframes)
X = diabetes_130_us_hospitals_for_years_1999_2008.data.features
y = diabetes_130_us_hospitals_for_years_1999_2008.data.targets

# metadata
print(diabetes_130_us_hospitals_for_years_1999_2008.metadata)

# variable information
print(diabetes_130_us_hospitals_for_years_1999_2008.variables)

# df = ...  # you said you already load & integrate the dataset
# Example target: readmitted (<=30days vs. other); adapt to your label

df = pd.concat([X, y], axis=1)
df = df.copy()
df['y'] = (df['readmitted'] == '<30').astype(int)

# Protected attributes (adapt to your cleaned columns)
prot_cols = ['age', 'race', 'gender']   # add 'weight' if reliably populated
A = df[prot_cols].copy()
y = df['y'].copy()

# Drop leakage/id columns as needed; define your model features
drop_cols = ['encounter_id','patient_nbr','readmitted','y'] + prot_cols
X = df.drop(columns=[c for c in drop_cols if c in df.columns])

X_train, X_temp, y_train, y_temp, A_train, A_temp = train_test_split(
    X, y, A, test_size=0.3, stratify=y, random_state=42
)
X_val, X_test, y_val, y_test, A_val, A_test = train_test_split(
    X_temp, y_temp, A_temp, test_size=0.5, stratify=y_temp, random_state=42
)

num_cols = X.select_dtypes(include=['number']).columns.tolist()
cat_cols = [c for c in X.columns if c not in num_cols]

pre = ColumnTransformer(
    transformers=[
        ('num', Pipeline([('impute', SimpleImputer(strategy='median')),
                          ('scale', StandardScaler())]), num_cols),
        ('cat', Pipeline([('impute', SimpleImputer(strategy='most_frequent')),
                          ('oh', OneHotEncoder(handle_unknown='ignore'))]), cat_cols),
    ]
)

# --- 0) assumes you already have: X_train, X_val, y_train, y_val, A_train, A_val, `pre` (transformer) ---
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

from ucimlrepo import fetch_ucirepo

# fetch dataset
dataset_id = 296
diabetes_130_us_hospitals_for_years_1999_2008 = fetch_ucirepo(id=dataset_id)

# data (as pandas dataframes)
X = diabetes_130_us_hospitals_for_years_1999_2008.data.features
y = diabetes_130_us_hospitals_for_years_1999_2008.data.targets

df = pd.concat([X, y], axis=1)
df = df.copy()
df['y'] = (df['readmitted'] == '<30').astype(int)

# Protected attributes (adapt to your cleaned columns)
prot_cols = ['age', 'race', 'gender']   # add 'weight' if reliably populated
A = df[prot_cols].copy()
y = df['y'].copy()

# Drop leakage/id columns as needed; define your model features
drop_cols = ['encounter_id','patient_nbr','readmitted','y'] + prot_cols
X = df.drop(columns=[c for c in drop_cols if c in df.columns])

X_train, X_temp, y_train, y_temp, A_train, A_temp = train_test_split(
    X, y, A, test_size=0.3, stratify=y, random_state=42
)
X_val, X_test, y_val, y_test, A_val, A_test = train_test_split(
    X_temp, y_temp, A_temp, test_size=0.5, stratify=y_temp, random_state=42
)

num_cols = X.select_dtypes(include=['number']).columns.tolist()
cat_cols = [c for c in X.columns if c not in num_cols]

pre = ColumnTransformer(
    transformers=[
        ('num', Pipeline([('impute', SimpleImputer(strategy='median')),
                          ('scale', StandardScaler())]), num_cols),
        ('cat', Pipeline([('impute', SimpleImputer(strategy='most_frequent')),
                          ('oh', OneHotEncoder(handle_unknown='ignore'))]), cat_cols),
    ]
)

# Install fairlearn
!pip install fairlearn

# Re-import fairlearn after installation
from fairlearn.metrics import MetricFrame, selection_rate, true_positive_rate, false_positive_rate, \
                               demographic_parity_difference, equalized_odds_difference
from sklearn.metrics import accuracy_score

def make_sample_weights(A_train_col, y_train):
    A_f = A_train_col.fillna("Unknown")
    p_y = y_train.value_counts(normalize=True)                   # P(Y)
    ct = pd.crosstab(A_f, y_train, normalize='index')            # P(Y|A=a)
    w = (ct.apply(lambda col: p_y[col.name] / col)).stack()      # w(a,y)=P(Y)/P(Y|A)
    return w.to_dict()                                           # keys: (a,y)

def map_weights(A_col, y, wdict):
    A_f = A_col.fillna("Unknown")
    return [wdict.get((a, int(t)), 1.0) for a, t in zip(A_f, y)]

def train_eval_for(A_key):
    # 1) weights
    wdict = make_sample_weights(A_train[A_key], y_train)
    sw_train = map_weights(A_train[A_key], y_train, wdict)
    # 2) train RF
    pipe = Pipeline([('pre', pre),
                     ('rf', RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1))])
    pipe.fit(X_train, y_train, rf__sample_weight=sw_train)
    # 3) eval (0.5 threshold; adjust if you use AUC/opt thresholding)
    y_pred = (pipe.predict_proba(X_val)[:,1] >= 0.5).astype(int)
    groups = A_val[A_key].fillna("Unknown")
    mf = MetricFrame(
        metrics={'acc': accuracy_score, 'sel_rate': selection_rate,
                 'tpr': true_positive_rate, 'fpr': false_positive_rate},
        y_true=y_val, y_pred=y_pred, sensitive_features=groups
    )
    dp = demographic_parity_difference(y_true=y_val, y_pred=y_pred, sensitive_features=groups)
    eo = equalized_odds_difference(y_true=y_val, y_pred=y_pred, sensitive_features=groups)
    print(f"\n== {A_key.upper()} ==")
    print("Overall acc:", accuracy_score(y_val, y_pred))
    print("By-group:\n", mf.by_group)
    print("DP diff:", dp, " | EO diff:", eo)

for A_key in ['race', 'gender', 'age']:
    train_eval_for(A_key)